{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRILL: Third Attempt\n",
    "\n",
    "So here's your task. Get rid of as much data as possible without dropping below an average of 90% accuracy in a 10-fold cross validation.\n",
    "\n",
    "You'll want to do a few things in this process. First, dive into the data that we have and see which features are most important. This can be the raw features or the generated dummies. You may want to use PCA or correlation matrices.\n",
    "\n",
    "Can you do it without using anything related to payment amount or outstanding principal? How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/y2015.csv', header=0)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df[:-2]\n",
    "#y2015.to_csv('data/y2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop0 = [label for label in df.columns if len(df[label].unique()) >= 100]\n",
    "drop1 = list(set(drop1) - set(drop0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(drop0, axis=1)\n",
    "df = df.drop(drop1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nan = df.columns[np.where(df.isna().any())]\n",
    "col_flt = df.columns[np.where(df.dtypes == 'float64')]\n",
    "col_obt = df.columns[np.where(df.dtypes == 'object')]\n",
    "col_int = df.columns[np.where(df.dtypes == 'int64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60 months</td>\n",
       "      <td>C5</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Current</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Credit card refinancing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36 months</td>\n",
       "      <td>A4</td>\n",
       "      <td>8 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Current</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Credit card refinancing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36 months</td>\n",
       "      <td>A4</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36 months</td>\n",
       "      <td>A2</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Current</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36 months</td>\n",
       "      <td>E3</td>\n",
       "      <td>8 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         term sub_grade emp_length home_ownership verification_status  \\\n",
       "0   60 months        C5  10+ years       MORTGAGE        Not Verified   \n",
       "1   36 months        A4    8 years       MORTGAGE        Not Verified   \n",
       "2   36 months        A4  10+ years       MORTGAGE        Not Verified   \n",
       "3   36 months        A2  10+ years       MORTGAGE        Not Verified   \n",
       "4   36 months        E3    8 years           RENT            Verified   \n",
       "\n",
       "    issue_d loan_status             purpose                    title  \\\n",
       "0  Dec-2015     Current         credit_card  Credit card refinancing   \n",
       "1  Dec-2015     Current         credit_card  Credit card refinancing   \n",
       "2  Dec-2015  Fully Paid  debt_consolidation       Debt consolidation   \n",
       "3  Dec-2015     Current  debt_consolidation       Debt consolidation   \n",
       "4  Dec-2015  Fully Paid  debt_consolidation       Debt consolidation   \n",
       "\n",
       "   delinq_2yrs  ...  num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  \\\n",
       "0          0.0  ...            9.0           11.0                  9.0   \n",
       "1          0.0  ...            4.0            4.0                  3.0   \n",
       "2          0.0  ...            5.0            9.0                  3.0   \n",
       "3          0.0  ...           13.0           18.0                  9.0   \n",
       "4          0.0  ...           17.0           17.0                 13.0   \n",
       "\n",
       "  num_sats num_tl_120dpd_2m  num_tl_30dpd  num_tl_90g_dpd_24m  \\\n",
       "0     11.0              0.0           0.0                 0.0   \n",
       "1      7.0              0.0           0.0                 0.0   \n",
       "2      9.0              0.0           0.0                 0.0   \n",
       "3     16.0              0.0           0.0                 0.0   \n",
       "4     18.0              NaN           0.0                 0.0   \n",
       "\n",
       "  num_tl_op_past_12m pub_rec_bankruptcies  tax_liens  \n",
       "0                2.0                  0.0        2.0  \n",
       "1                2.0                  0.0        0.0  \n",
       "2                0.0                  0.0        0.0  \n",
       "3                0.0                  0.0        0.0  \n",
       "4               12.0                  1.0        0.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  1.,  7.,  5.,  0., 10.,  4., 18.,  9., 15., 11.,  6.,\n",
       "       14., 12., 19.,  8., 17., 21., 16., 13., 24., 20., 30., 23., 25.,\n",
       "       31., 22., 26., 33., 32., 34., nan, 27., 28., 40.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.open_il_6m.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['term','int_rate'] # digits only\n",
    "b = ['subgrade', 'emp_length', 'application_type'] #dict transform\n",
    "c = ['home_ownership', 'loan_status', 'purpose', 'title'] #dict transform unranked\n",
    "d = ['issue_d', 'last_pymnt_d', 'last_credit_pull_d'] # time\n",
    "e = ['verification_status_joint', 'verification_status']# combine\n",
    "f = ['open_acc_6m']# nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
       "       'collections_12_mths_ex_med', 'policy_code', 'acc_now_delinq',\n",
       "       'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',\n",
       "       'open_rv_12m', 'open_rv_24m', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
       "       'acc_open_past_24mths', 'chargeoff_within_12_mths', 'mort_acc',\n",
       "       'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl',\n",
       "       'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_op_rev_tl',\n",
       "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m',\n",
       "       'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
       "       'pub_rec_bankruptcies', 'tax_liens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_flt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['emp_length', 'title', 'last_pymnt_d', 'last_credit_pull_d',\n",
    "       'verification_status_joint', '', '', 'open_il_12m',\n",
    "       'open_il_24m', 'open_rv_12m', 'open_rv_24m', 'inq_fi', 'total_cu_tl',\n",
    "       'inq_last_12m', 'mths_since_recent_inq', 'num_rev_accts',\n",
    "       'num_tl_120dpd_2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1 = ['grade', 'emp_title', 'pymnt_plan', 'url', 'desc','zip_code', 'addr_state', 'earliest_cr_line', \n",
    "          'initial_list_status', 'next_pymnt_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=col_obt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### object columns\n",
    "- 'id' is unique id for each loan\n",
    "- 'member_id' is unique id for person taking out loan\n",
    "- 'emp_title' too many values, utility doubted\n",
    "- 'pymnt_plan' has only one value\n",
    "- 'url' is unique page for every loan\n",
    "- 'desc' too many unique values\n",
    "- 'grade' is further broken down to in sub_grade\n",
    "- 'subgrade\n",
    "- 'zip_code' too many unique values\n",
    "- 'pymnt_plan' only one value\n",
    "\n",
    "#### NaN columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn['nan02'] = np.where(dfn.percent_bc_gt_75.isna(), 'nan', 'not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.countplot(x='loan_status', data=dfn, hue='nan02',  ax=ax)\n",
    "#ax.set(ylabel=doz['name'], xlabel=doz['xlabel'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nft = list(set(col_nan) & set(col_flt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = dfn.groupby('loan_status').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.loan_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn[['loan_status', 'mths_since_last_record', 'mths_since_last_delinq']][dfn.mths_since_last_delinq == np.nan].groupby('loan_status').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series({col : len(np.asarray(getattr(dfn,col).isna()).nonzero()[0].tolist()) / len(dfn) * 100 for col in col_nft})\n",
    "B = pd.Series({col : len(np.asarray(getattr(dfn,col).unique())) for col in col_nft})\n",
    "C = pd.Series({col : np.asarray(getattr(dfn,col).max()) for col in col_nft})   \n",
    "D = pd.Series({col : np.asarray(getattr(dfn,col).min()) for col in col_nft}) \n",
    "df1 = pd.DataFrame([A,B,C,D]).T\n",
    "df1.columns = ['nans', 'unique','max', 'min']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.nans <= 94].T.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn[dfn.nans >= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.mths_since_last_major_derog.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    " [np.where(getattr(dfn,col).isna()) for col in col_nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['emp_title', 'pymnt_plan', 'desc', 'url','title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(col, len(y2015[col].unique())) for col in col_obt if len(y2015[col].unique()) <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015['int_rate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y2015['int_rate'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.loc[np.where(y2015['last_pymnt_d'].isna())]['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ['Jan-2017', 'Sep-2016', 'May-2016', 'Mar-2016', 'Jul-2016',\n",
    "       'Jun-2016', 'Nov-2016', 'Aug-2016', 'Dec-2016', 'Oct-2016',\n",
    "       'Apr-2016', 'Feb-2016', 'Jan-2016', 'Dec-2015', 'Nov-2015',\n",
    "       'Oct-2015', 'Sep-2015', 'Aug-2015', 'Jul-2015', 'Jun-2015',\n",
    "       'May-2015', 'Apr-2015', 'Mar-2015', 'Feb-2015', 'Jan-2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015['last_pymnt_d'][list(set(np.array(y2015.index)) - set(np.where(y2015['last_pymnt_d'].isin(ts))[0]) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y2015.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015[y2015['last_pymnt_d'] == 'Jan-2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Timestamp('Jan-2015') - pd.Timestamp('Jan-2015')).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(pd.Timestamp(ts) - pd.Timestamp('Jan-2015')).days for ts in y2015['last_pymnt_d'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = [('grade',     {'GFEDCBA'[i]:i for i in range(6)}),\n",
    "         ('sub_grade', {[char + str(i) for char in 'GFEDCBA' for i in range(5,0,-1)][i]:i for i in range(34,-1,-1)}),\n",
    "         ('verification_status', {['Not Verified', 'Verified', 'Source Verified'][i]:i for i in range(3)}),   \n",
    "         ('issue_d ', {['Dec-2015', 'Nov-2015', 'Oct-2015', 'Sep-2015', 'Aug-2015','Jul-2015', 'Jun-2015', 'May-2015',\n",
    "          'Apr-2015', 'Mar-2015', 'Feb-2015', 'Jan-2015'][i]:i for i in range(12)}),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal rank based upon target mean\n",
    "def ordinal_by_target(df, tgt, fte):\n",
    "    rnk  = df.groupby(fte)[tgt].mean().sort_values(ascending=True).index\n",
    "    return {rnk[i]:i for i in range(len(rnk))}\n",
    "# functions to clean or tranform columns\n",
    "# transform using dictionary\n",
    "def A(srs, arg): \n",
    "    assert len(arg) == 1\n",
    "    return srs.apply(lambda x : arg[0][x])\n",
    "# linear scale features to approximate range -2 to 2\n",
    "def B(srs, arg):  \n",
    "    rng = srs.max()-srs.min()\n",
    "    mid = srs.max() - rng / 2\n",
    "    return srs.apply(lambda x : (x - mid) * 4 / rng)\n",
    "# standardize feature with z score\n",
    "def C(srs, arg):\n",
    "    z_score = lambda x: (x-x.mean())/x.std()\n",
    "    return srs.transform(z_score )    \n",
    "\n",
    "# dict switch to call funtions\n",
    "switch = {char:eval(char) for char in 'ABC'}\n",
    "#\n",
    "def switch_clean(fnn, srs, arg):\n",
    "    for f in fnn:       \n",
    "         srs = switch[f](srs,arg)               \n",
    "    return srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new columns \n",
    "df1['passed'] = np.where(df1.a_scre > 0, 1, 0)\n",
    "df1['g_avg_'] = np.round(df1.g_tl_s.div(df1.g_tl_n), decimals = 4)\n",
    "# transform columns\n",
    "df1.ise_id =  df1.ise_id.add(df1.lea_id * 100)\n",
    "df1.age_mh =  df1.age_mh.add(222)\n",
    "# dicts to put features in rank order by target ascending\n",
    "obts = {x:ordinal_by_target(df1, 'a_scre', x ) for x in a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(S): # strip()\n",
    "    return S.apply(lambda x: x.str.strip())\n",
    "def B(S): # conversion to numeric\n",
    "    dct = {S.unique()[i]:i for i in range(len(S.unique()))}\n",
    "    return S.apply(lambda x: dct[x])\n",
    "def B(S): # cast to numeric\n",
    "    return pd.to_numeric(S) \n",
    "def B(S):  # cast float to int\n",
    "    return S.apply(lambda x: int(x))\n",
    "def C(S): # extract by regex 1 or more digits\n",
    "    return S.apply(lambda x: x.str.extract(r\"\\b\\d+\\b\")\n",
    "def D(S): # replace all non-alpha numeric with ' '\n",
    "    return S.apply(lambda x: x.str.replace(r'\\W+', ' ', regex=True))\n",
    "def E(S): # numeric conversion letter grade A-G \n",
    "    dct = {'GFEDCBA'[i]:i for i in range(6)}\n",
    "    return S.apply(lambda x: dct[x]) \n",
    "def E(S): # numeric conversion letter grade A1-G5 \n",
    "    dct = {[char + str(i) for char in 'GFEDCBA' for i in range(5,0,-1)][i]:i for i in range(34,-1,-1)}\n",
    "    return S.apply(lambda x: dct[x])                   \n",
    "def D(S): # replace the  '< 1' with '0' for years\n",
    "    return S.apply(lambda x: x.str.replace('< 1', '0')                   \n",
    "                   \n",
    "def E(S): return S.apply(lambda x: x.str.replace(r'\\s{2}', ' ', regex=True)) # replace 2 or more white space with 1\n",
    "\n",
    "\n",
    "\n",
    "def B(S): return S.apply(lambda x: x.str.replace(r\"\\Bn't\\b\", ' not ', regex=True), axis=1)\n",
    "def C(S): return S.apply(lambda x: x.str.replace(r'\\W+', ' ', regex=True), axis=1)\n",
    "def D(S): return S.apply(lambda x: x.str.replace(r'\\d+', ' ', regex=True), axis=1)\n",
    "def E(S): return S.apply(lambda x: x.str.lower(), axis=1) \n",
    "def F(S): return S.apply(lambda x: x.str.replace(r\"\\bnot\\s\", 'not', regex=True), axis=1)\n",
    "def G(S): return S.apply(lambda x: x.str.replace(r'\\s{2}', ' ', regex=True), axis=1)  \n",
    "def H(S): return S.apply(lambda x: x.str.strip(), axis=1)\n",
    "def I(S): return S.apply(lambda x: x.str.split(), axis=1)\n",
    "def Z(S): return S\n",
    "switch = {char:eval(char) for char in 'ABCDEFGHI'}\n",
    "\n",
    "def switch_clean_column(S, funcs):\n",
    "    for fun in funcs:\n",
    "        S = switch[fun](S)              \n",
    "    return S \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [{k:v} for k,v in vks]\n",
    "for dct in dicts:\n",
    "    switch_clean_column(getattr(df, dct['col']), dct['funcs'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',\n",
    "       'mths_since_last_record', 'open_acc', 'pub_rec', ]# to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(S):  # cast float to int\n",
    "    return S.apply(lambda x: int(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['emp_title', 'pymnt_plan', 'desc', 'url']# drop\n",
    "['A']\n",
    "['BC']['term', ]\n",
    "['BD']['int_rate', ]\n",
    "['']['issue_d', 'purpose', 'title']\n",
    "['emp_length'] # unique cleaning\n",
    "['dti']# floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.total_acc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y2015.delinq_2yrs.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## The Blind Approach\n",
    "\n",
    "Now, as we've seen before, creating a model is the easy part. Let's try just using everything we've got and throwing it without much thought into a Random Forest. SKLearn requires the independent variables to be be numeric, and all we want is dummy variables so let's use `get_dummies` from Pandas to generate a dummy variable for every categorical column and see what happens off of this kind of naive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Did your kernel die? My kernel died.\n",
    "\n",
    "Guess it isn't always going to be that easy...\n",
    "\n",
    "Can you think of what went wrong?\n",
    "\n",
    "(You're going to have to reset your kernel and reload the column, BUT DON'T RUN THE MODEL AGAIN OR YOU'LL CRASH THE KERNEL AGAIN!)\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Well, `get_dummies` can be a very memory intensive thing, particularly if data are typed poorly. We got a warning about that earlier. Mixed data types get converted to objects, and that could create huge problems. Our dataset is about 400,000 rows. If there's a bad type there its going to see 400,000 distinct values and try to create dummies for all of them. That's bad. Lets look at all our categorical variables and see how many distinct counts there are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = y2015.select_dtypes(include=['float'])\n",
    "len(floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y2015.total_pymnt.astype(str) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y2015.dti.astype(str).str.split(pat='.')[1] == '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y2015.dti.astype(str).str.split(pat='.', expand=True)[1] == '0')[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical = y2015.select_dtypes(include=['object'])\n",
    "for i in categorical:\n",
    "    column = categorical[i]\n",
    "    print(i)\n",
    "    print(column.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well that right there is what's called a problem. Some of these have over a hundred thousand distinct types. Lets drop the ones with over 30 unique values, converting to numeric where it makes sense. In doing this there's a lot of code that gets written to just see if the numeric conversion makes sense. It's a manual process that we'll abstract away and just include the conversion.\n",
    "\n",
    "You could extract numeric features from the dates, but here we'll just drop them. There's a lot of data, it shouldn't be a huge problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert ID and Interest Rate to numeric.\n",
    "y2015['id'] = pd.to_numeric(y2015['id'], errors='coerce')\n",
    "y2015['int_rate'] = pd.to_numeric(y2015['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "y2015.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonder what was causing the dtype error on the id column, which _should_ have all been integers? Let's look at the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove two summary rows at the end that don't actually contain data.\n",
    "y2015 = y2015[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now this should be better. Let's try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "It finally works! We had to sacrifice sub grade, state address and description, but that's fine. If you want to include them you could run the dummies independently and then append them back to the dataframe.\n",
    "\n",
    "## Second Attempt\n",
    "\n",
    "Now let's try this model again.\n",
    "\n",
    "We're also going to drop NA columns, rather than impute, because our data is rich enough that we can probably get away with it.\n",
    "\n",
    "This model may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y2015.id.dtype == 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015.apply(lambda x: x.isnull().any(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "X = y2015.drop('loan_status', 1)\n",
    "Y = y2015['loan_status']\n",
    "X = pd.get_dummies(X)\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The score cross validation reports is the accuracy of the tree. Here we're about 98% accurate.\n",
    "\n",
    "That works pretty well, but there are a few potential problems. Firstly, we didn't really do much in the way of feature selection or model refinement. As such there are a lot of features in there that we don't really need. Some of them are actually quite impressively useless.\n",
    "\n",
    "There's also some variance in the scores. The fact that one gave us only 93% accuracy while others gave higher than 98 is concerning. This variance could be corrected by increasing the number of estimators. That will make it take even longer to run, however, and it is already quite slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
