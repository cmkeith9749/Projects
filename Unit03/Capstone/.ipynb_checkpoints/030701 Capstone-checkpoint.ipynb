{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Prices with Ames Data Set\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data context](#data)\n",
    "3. [Initial Model Run](#initial)\n",
    "3. [Exploratory Analysis](#explore)   \n",
    "4. [Data Cleaning](#clean)\n",
    "5. [Feature Engineering](#engineer)\n",
    "6. [Algorithm Selection](#select)\n",
    "6. [Hyperparameter Tuning](#tune)\n",
    "7. [Model Training](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utilities as mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand Problem<a name=\"data\"></a>\n",
    "- clean enough to plot \n",
    "- preliminary resolution of missing values\n",
    "- categorize features\n",
    "- understand target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'AmesHousing/Data/train.csv'\n",
    "df = pd.read_csv(file) \n",
    "df.columns = [mut.label_uncap_split(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "#df.dtypes\n",
    "#df.columns\n",
    "#df.describe()\n",
    "#uniq = {col:len(df[col].unique()) for col in df if col in col_obj}\n",
    "#df.loc[np.where(df.mas_vnr_area.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan for plotting\n",
    "col_obj = [col for col in df if pd.api.types.is_object_dtype(df[col])]\n",
    "col_num = [col for col in df if not col in col_obj]\n",
    "col_nan = [col for col in df if df[col].isna().any()]\n",
    "col_onn =  set(col_obj) & set(col_nan)\n",
    "col_fnn =  set(col_num) & set(col_nan)\n",
    "col_nnn =  set(col_nan) - set(col_onn)\n",
    "for col in col_obj:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "for col in col_fnn:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,6))\n",
    "sns.distplot(df.sale_price, color=\"b\", kde = False,  ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial model run<a name=\"initial\"></a>\n",
    "Run a simple model to help define our problem.  Maybe that is all that is needed. The results can guide the feature engineering process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data and define categorical and numerical data\n",
    "file = 'AmesHousing/Data/train.csv'\n",
    "dfn = pd.read_csv(file) \n",
    "dfn.columns = [mut.label_uncap_split(col) for col in dfn.columns]\n",
    "col_cat = [col for col in dfn if pd.api.types.is_object_dtype(dfn[col])]\n",
    "col_num = [col for col in dfn if not col in col_cat]\n",
    "col_num.remove('id'); col_num.remove('sale_price')\n",
    "X = dfn.drop(['id', 'sale_price'], axis=1)\n",
    "y = dfn['sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, X_tst, y_trn, y_tst = train_test_split( X, y, test_size=0.10, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "num_tfr = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('ssr', StandardScaler())])\n",
    "\n",
    "cat_tfr = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "ppr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_tfr, col_num),\n",
    "        ('cat_tfr', cat_tfr, col_cat)])\n",
    "\n",
    "est_ols = Pipeline(steps=[('ppr', ppr),\n",
    "                      ('est', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prd = est_ols.fit(X_trn, y_trn).predict(X_tst)\n",
    "print(\"R Squared: %.3f\" % r2_score(y_prd, y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predicted values.\n",
    "predicted = y_prd.ravel()\n",
    "actual = y_tst\n",
    "# Calculate the error, also called the residual.\n",
    "residual = actual - predicted\n",
    "plt.hist(residual)\n",
    "plt.title('Residual counts');plt.xlabel('Residual');plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predicted, residual)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "f, ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(correlation_matrix, vmax=.8, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results were disappointingly good.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Analysis\n",
    "\n",
    " - Start with Basics\n",
    "\n",
    " - Context of Data\n",
    "\n",
    " - Plot Numerical Distributions\n",
    " \n",
    " - Plot Categorical Distributions\n",
    "\n",
    " - Plot Segmentations\n",
    "\n",
    " - Study Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'AmesHousing/Data/train.csv'\n",
    "dfe = pd.read_csv(file) \n",
    "dfe.columns = [mut.label_uncap_split(col) for col in dfe.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cat = [col for col in dfe.columns if dfe[col].dtype == 'O']\n",
    "col_flt = [col for col in dfe.columns if dfe[col].dtype == 'float64']\n",
    "col_int = [col for col in dfe.columns if dfe[col].dtype == 'int64']\n",
    "col_nan = [col for col in dfe.columns if dfe[col]a.isna().any()]\n",
    "col_ord = [col for col in col_int if len(dfe[col].unique()) <= 12]\n",
    "col_num = list(set(col_int) - set(col_ord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = [col for col in col_obj if (len(df[col].unique()) <= 8)]\n",
    "plot2 = [col for col in col_obj if (len(df[col].unique()) > 8)]\n",
    "plot2.remove('neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_cnn:\n",
    "    dfe[col] = dfe[col].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_onn =  set(col_obj) & set(col_nan)\n",
    "col_fnn =  set(col_num) & set(col_nan)\n",
    "col_nnn =  set(col_nan) - set(col_onn)\n",
    "for col in col_obj:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "for col in col_fnn:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_onn =  set(col_obj) & set(col_nan)\n",
    "col_fnn =  set(col_num) & set(col_nan)\n",
    "col_nnn =  set(col_nan) - set(col_onn)\n",
    "for col in col_obj:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "for col in col_fnn:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = len(plot1) // cols \n",
    "fig, axs = plt.subplots(rows, cols, sharex='col', sharey='row', figsize=(16, rows * 4))\n",
    "i = 0\n",
    "for x in range(rows):\n",
    "    for y in range(cols):\n",
    "        sns.countplot(x=plot1[i], data=df, ax=axs[x][y])     \n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = len(plot1) // cols \n",
    "fig, axs = plt.subplots(rows, cols, sharex='col', sharey='row', figsize=(16, rows * 4))\n",
    "i = 0\n",
    "for x in range(rows):\n",
    "    for y in range(cols):\n",
    "        sns.barplot(x=plot1[i], y=\"sale_price\", data=df, ax=axs[x][y])     \n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 1\n",
    "rows = len(plot2) // cols \n",
    "fig, axs = plt.subplots(rows, cols, sharex='col', sharey='row', figsize=(16, rows * 5))\n",
    "\n",
    "for i in range( rows ):\n",
    "        sns.barplot(x=plot2[i], y=\"sale_price\", data=df, ax=axs[i])     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 24))\n",
    "sns.barplot(x=\"sale_price\", y='neighborhood', data=df, orient='h', ax=ax)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,6))\n",
    "sns.distplot(df.lot_frontage, color=\"b\", kde = False, norm_hist=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = len(col_num) // cols \n",
    "fig, axs = plt.subplots(rows, cols, sharex='col', sharey='row', figsize=(16, rows * 5))\n",
    "i = 0\n",
    "for x in range(rows):\n",
    "    for y in range(cols):\n",
    "        sns.distplot(df[col_num[i]],  ax=axs[x][y])     \n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning<a name=\"clean\"></a>\n",
    " - Remove Unwanted observations\n",
    " - Fix Structural Errors\n",
    " - Filter Unwanted Outliers\n",
    " - Handle Missing Data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering<a name=\"engineer\"></a>\n",
    "\n",
    "- Transform Target\n",
    "\n",
    "- Infuse Domain Knowledge\n",
    "\n",
    "- Create Interaction Features\n",
    " \n",
    "- Combine Sparse Classes\n",
    "\n",
    "- Add Dummy Variables \n",
    "\n",
    "- Remove Unused Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Feature Engineering 1\n",
    " - correct feature classifications\n",
    " - add interaction feature to account for time related variance\n",
    " - rank the ordinal quality and condition values\n",
    " - acount for outliers in continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'AmesHousing/Data/train.csv'\n",
    "dff = pd.read_csv(file) \n",
    "dff.columns = [mut.label_uncap_split(col) for col in dff.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize features for preprocessing, correct errors from initial model \n",
    "fte_cat = [col for col in dff.columns if dff[col].dtype == 'O']\n",
    "fte_flt = [col for col in dff.columns if dff[col].dtype == 'float64']\n",
    "fte_int = [col for col in dff.columns if dff[col].dtype == 'int64']\n",
    "fte_nan = [col for col in dff.columns if dff[col].isna().any()]\n",
    "#fte_ord = [col for col in fte_int if len(dff[col].unique()) <= 12]\n",
    "drop1 = ['id', 'sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass is categorical feature with integers for values.  change to letters to avoid confusion again\n",
    "A = dff.sub_class.unique()\n",
    "dct_scs = {A[i]:chr for (i, chr) in enumerate('ABCDEFGHIJKLMNO')}\n",
    "dff.sub_class =  mut.fillna_transform_dct(dff.sub_class, dct_scs)\n",
    "fte_cat.append('sub_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.garage_yr_blt  = 2016 - dff.garage_yr_blt\n",
    "dff.year_built     = 2016 - dff.year_built\n",
    "dff.year_remod_add = 2016 - dff.year_remod_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ord_mth for variance across entire time period\n",
    "dff['ord_mth'] = (dff.yr_sold - 2006) * 12 + dff.mo_sold\n",
    "fte_tme = ['ord_mth']; drop1.append('yr_sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality and condition ranks to match 1-10 scale for overall condition \n",
    "dct_ord = {'Ex':9,'Gd':7, 'TA':5, 'Fa':3, 'Po':2, 'NA':0}\n",
    "fte_ord = ['overall_qual','overall_cond']\n",
    "tfm_ord = ['bsmt_qual', 'bsmt_cond', 'extr_qual', 'extr_cond','heating_qc', 'garage_qual', 'garage_cond','pool_qc' ]\n",
    "\n",
    "for tfm in tfm_ord:\n",
    "    dff[tfm] = mut.fillna_transform_dct(dff[tfm], dct_ord)\n",
    "    fte_ord.append(tfm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_cat = list(set(fte_cat) - set(fte_ord)) \n",
    "fte_cnt = list(set(fte_flt) | set(fte_int) - set(fte_ord) -  set(fte_cat))\n",
    "fte_ord = list(set(fte_ord) | set(fte_tme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_cat = [fte for fte in fte_cat if not fte in drop1]\n",
    "fte_cnt = [fte for fte in fte_cnt if not fte in drop1]\n",
    "fte_ord = [fte for fte in fte_ord if not fte in drop1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Selection <a name=\"select\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tfr_cnt = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('ssr', StandardScaler()),\n",
    "    ('mms', MinMaxScaler())])\n",
    "tfr_ord = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('mms', MinMaxScaler())])\n",
    "tfr_cat = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "ppr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cnt', tfr_cnt, fte_cnt),\n",
    "        ('cat', tfr_cat, fte_cat),\n",
    "        ('ord', tfr_ord, fte_ord)])\n",
    "\n",
    "est_ols = Pipeline(steps=[('ppr', ppr), ('est', LinearRegression())])\n",
    "est_lso = Pipeline(steps=[('ppr', ppr), ('est', Lasso())])\n",
    "est_rdg = Pipeline(steps=[('ppr', ppr), ('est', Ridge())])\n",
    "est_ent = Pipeline(steps=[('ppr', ppr), ('est', ElasticNet())])\n",
    "est_svr = Pipeline(steps=[('ppr', ppr), ('est', SVR())])\n",
    "est_svr = Pipeline(steps=[('ppr', ppr), ('est', KernelRidge())])\n",
    "est_gbr = Pipeline(steps=[('ppr', ppr), ('est', GradientBoostingRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dff.drop(drop1, axis=1)\n",
    "y = dff['sale_price']\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split( X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = [est_ols, est_lso, est_rdg, est_ent, est_svr, est_svr, est_gbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prd = est_ols.fit(X_trn, y_trn).predict(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_tst, y_prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"log mean squared error %.8f\" % mean_squared_log_error( y_tst, y_prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds = [est.fit(X_trn, y_trn).predict(X_tst) for est in ests]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2s = [str(i) + ' : ' + \"R Squared: %.3f\" % r2_score(y_tst, prd) for (i,prd) in enumerate(prds)]\n",
    "mses = [str(i) + ' : ' + \"mean squared log error: %.3f\" % mean_squared_log_error(y_tst, prd)\n",
    "        for (i,prd) in enumerate(prds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_2 in r_2s:\n",
    "    print(r_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mse in mses:\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering 2\n",
    "  feature engineering 2\n",
    " - combine the sparse classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop2 = drop1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['bath'] = dff.half_bath * 2 + dff.full_bath * 4 + dff.bsmt_half_bath + dff.bsmt_full_bath\n",
    "fte_cnt.append('bath'); drop2 = drop2 + ['half_bath', 'full_bath', 'bsmt_half_bath', 'bsmt_full_bath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine condition1 and condition2 into ordinal feature\n",
    "dct_cdn = {'Artery':-3, 'Feedr':-1, 'Norm':0,'RRNn':-1, 'RRAn':-3, 'PosN':1, 'PosA':3,'RRNe':-1, 'RRAe':-3}\n",
    "dff.condition1 = dff.condition1.apply(lambda x : dct_cdn[x])\n",
    "dff.condition2 = dff.condition2.apply(lambda x : dct_cdn[x])\n",
    "dff['cndn'] = dff.condition1 + dff.condition2\n",
    "fte_ord.append('cndn'); drop2 = drop2 + ['condition1', 'condition2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for variance in \"low_qual_fin_sf\" with ratio and delete redundent first and second floor areas\n",
    "dff['gr_liv_qlty'] = (dff.gr_liv_area - dff.low_qual_fin_sf) / dff.gr_liv_area\n",
    "fte_cnt.append('gr_liv_qlty'); drop2 = drop2 + ['frst_flr_sf', 'scnd_flr_sf', 'low_qual_fin_sf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['gar_cq'] = dff.garage_cond + dff.garage_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_ord.append('gr_liv_qlty'); drop2 = drop2 + ['garage_cond', 'garage_qual','garage_area' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       'bsmt_qual', 'bsmt_cond', 'bsmt_exposure', 'bsmt_fin_type1',\n",
    "       'bsmt_fin_sf1', 'bsmt_fin_type2', 'bsmt_fin_sf2', 'bsmt_unf_sf','total_bsmt_sf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['bmt_cq'] = dff.bsmt_cond + dff.bsmt_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_bmt = {'GLQ':20, 'ALQ':17, 'Unf':3, 'Rec':11, 'BLQ':14, 'NA':0, 'LwQ':7}\n",
    "dff.bsmt_fin_type1 =  mut.fillna_transform_dct(dff.bsmt_fin_type1, dct_bmt)\n",
    "dff.bsmt_fin_type2 =  mut.fillna_transform_dct(dff.bsmt_fin_type2, dct_bmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['bmt_ttl'] = (dff.bsmt_fin_type1 * dff.bsmt_fin_sf1 + \n",
    "                  dff.bsmt_fin_type2 * dff.bsmt_fin_sf2 + dff.bsmt_unf_sf * 3) // 20\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_ord.append('bmt_cq'); fte_ord.append('bmt_ttl')\n",
    "drop2 = drop2 + ['bsmt_qual', 'bsmt_cond', 'bsmt_fin_type1', 'bsmt_fin_sf1',\n",
    "                 'bsmt_fin_type2', 'bsmt_fin_sf2', 'bsmt_unf_sf','total_bsmt_sf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_cat = [fte for fte in fte_cat if not fte in drop2]\n",
    "fte_cnt = [fte for fte in fte_cnt if not fte in drop2]\n",
    "fte_ord = [fte for fte in fte_ord if not fte in drop2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection<a name=\"tune\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tfr_cnt = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('ssr', StandardScaler()),\n",
    "    ('mms', MinMaxScaler())])\n",
    "tfr_ord = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('mms', MinMaxScaler())])\n",
    "tfr_cat = Pipeline(steps=[\n",
    "    ('sir', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "ppr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cnt', tfr_cnt, fte_cnt),\n",
    "        ('cat', tfr_cat, fte_cat),\n",
    "        ('ord', tfr_ord, fte_ord)])\n",
    "\n",
    "mdl = Pipeline(steps=[('ppr', ppr), ('est', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all the data to fit model\n",
    "mdl = mdl.fit(X,y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model in Production (sorta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'AmesHousing/Data/test.csv'\n",
    "dft = pd.read_csv(file) \n",
    "dft.columns = [mut.label_uncap_split(col) for col in dft.columns]\n",
    "tst_ids = dft.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize features for preprocessing, correct errors from initial model \n",
    "fte_cat = [col for col in dft.columns if dft[col].dtype == 'O']\n",
    "fte_flt = [col for col in dft.columns if dft[col].dtype == 'float64']\n",
    "fte_int = [col for col in dft.columns if dft[col].dtype == 'int64']\n",
    "fte_nan = [col for col in dft.columns if dft[col].isna().any()]\n",
    "#fte_ord = [col for col in fte_int if len(dft[col].unique()) <= 12]\n",
    "drop1 = ['id']\n",
    "# MSSubClass is categorical feature with integers for values.  change to letters to avoid confusion again\n",
    "A = dft.sub_class.unique()\n",
    "dct_scs = {A[i]:chr for (i, chr) in enumerate('ABCDEFGHIJKLMNOP')}\n",
    "dft.sub_class =  mut.fillna_transform_dct(dft.sub_class, dct_scs)\n",
    "fte_cat.append('sub_class')\n",
    "# ord_mth for variance across entire time period\n",
    "dft['ord_mth'] = (dft.yr_sold - 2006) * 12 + dft.mo_sold\n",
    "fte_tme = ['ord_mth']; drop1.append('yr_sold')\n",
    "# quality and condition ranks to match 1-10 scale for overall condition \n",
    "dct_ord = {'Ex':9,'Gd':7, 'TA':5, 'Fa':3, 'Po':2, 'NA':0}\n",
    "fte_ord = ['overall_qual','overall_cond']\n",
    "tfm_ord = ['bsmt_qual', 'bsmt_cond', 'extr_qual', 'extr_cond','heating_qc', 'garage_qual', 'garage_cond','pool_qc' ]\n",
    "\n",
    "for tfm in tfm_ord:\n",
    "    dft[tfm] = mut.fillna_transform_dct(dft[tfm], dct_ord)\n",
    "    fte_ord.append(tfm)  \n",
    "fte_cat = list(set(fte_cat) - set(fte_ord)) \n",
    "fte_cnt = list(set(fte_flt) | set(fte_int) - set(fte_ord) -  set(fte_cat))\n",
    "fte_ord = list(set(fte_ord) | set(fte_tme))\n",
    "fte_cat = [fte for fte in fte_cat if not fte in drop1]\n",
    "fte_cnt = [fte for fte in fte_cnt if not fte in drop1]\n",
    "fte_ord = [fte for fte in fte_ord if not fte in drop1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
