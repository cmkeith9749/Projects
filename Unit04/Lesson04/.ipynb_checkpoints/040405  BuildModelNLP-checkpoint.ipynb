{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Classifier\n",
    "1. [Objective](#obj)\n",
    "1. [Module Imports](#import)\n",
    "2. [Raw Text](#raw)\n",
    "3. [Tokenizer](#toke)\n",
    "4. [Feature](#feat)\n",
    "4. [Classifier](#classify)\n",
    "5. [Scratch](#scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective <a name=\"obj\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was **93%**.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "The best accuracy score was actually **0.8707**.  The spaCy and sklean in the online notebook is outdated. Plus it would not be much of a challenge to get above 90% from a score of 93%.\n",
    "\n",
    "Did not take much to get above 90%.  My \"bag of words\" was \"cleaner\" as I corrected the tokenizing errors.  Actually reducing the number words to 668 by only including in the bag of words increased the words \n",
    "\n",
    "\n",
    "logit with the default hyperparameters had the following [result](#scratch) shown below:\n",
    "\n",
    "{'logit': {'train': 0.9665653495440729, 'test': 0.9036144578313253}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'melville-moby_dick.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module Imports <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Text <a name=\"raw\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean guttenberg specific text\n",
    "def txt_clean(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "persn = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "#remove the chapter headings\n",
    "persn = re.sub(r'Chapter \\d+', '', persn)\n",
    "alice = re.sub(r'CHAPTER .*',  '', alice)\n",
    " \n",
    "alice = txt_clean(alice[:int(len(alice)/10)]) \n",
    "persn = txt_clean(persn[:int(len(persn)/10)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobyd = gutenberg.raw('melville-moby_dick.txt')\n",
    "mobyd = re.sub(r'Chapter \\d+', '',mobyd)\n",
    "mobyd = txt_clean(mobyd[:int(len(mobyd)/10)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "sense = re.sub(r'Chapter \\d+', '',sense)\n",
    "sense = txt_clean(sense[:int(len(sense)/10)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer <a name=\"toke\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# process the .sents further to eliminate those without words\n",
    "snt1 = [S for S in nlp(persn).sents if True in [s.is_alpha for s in S]] \n",
    "snt2 = [S for S in nlp(alice).sents if True in [s.is_alpha for s in S]] \n",
    "snt3 = [S for S in nlp(mobyd).sents if True in [s.is_alpha for s in S]]\n",
    "snt4 = [S for S in nlp(sense).sents if True in [s.is_alpha for s in S]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "astn = [[S,  len(S), \"Austen\"]   for S in snt1] \n",
    "crll = [[S,  len(S), \"Carroll\"]  for S in snt2]\n",
    "mlvl = [[S,  len(S), \"Melville\"] for S in snt3]\n",
    "snse = [[S,  len(S), \"Sense\"]    for S in snt4]\n",
    "\n",
    "dfs = pd.DataFrame((astn + crll),               columns = ['sentence','length','who'])\n",
    "df3 = pd.DataFrame((astn + crll + mlvl),        columns = ['sentence','length','who'])\n",
    "df4 = pd.DataFrame((astn + crll + mlvl + snse), columns = ['sentence','length','who'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features <a name=\"feat\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_process(S):\n",
    "    stops = STOP_WORDS\n",
    "    A = [s for s in S if s.is_alpha and not s.is_stop]\n",
    "    B = [a for a in A if a.lemma_ not in stops]\n",
    "    return [b.lemma_.lower() if b.pos_ != 'PROPN' else b.lower_ for b in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the processed sentences to create word list and count words\n",
    "F = [(sentence_process(S)) for S in dfs.sentence]\n",
    "assert len(dfs) == len(F)\n",
    "ctr = Counter()\n",
    "for f in F: ctr.update(f)\n",
    "fte = [k for (k,v) in ctr.most_common() if ctr[k] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the processed sentences to create word list and count words\n",
    "F = [(sentence_process(S)) for S in df3.sentence]\n",
    "assert len(df3) == len(F)\n",
    "ctr = Counter()\n",
    "for f in F: ctr.update(f)\n",
    "fts = [k for (k,v) in ctr.most_common() if ctr[k] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the processed sentences to create word list and count words\n",
    "F = [(sentence_process(S)) for S in df4.sentence]\n",
    "assert len(df4) == len(F)\n",
    "ctr = Counter()\n",
    "for f in F: ctr.update(f)\n",
    "ft4 = [k for (k,v) in ctr.most_common() if ctr[k] >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero dataframe length number of sentences and width number of words (1000)\n",
    "bow = pd.DataFrame(np.zeros((len(dfs), len(fte))), columns=fte, dtype=int)\n",
    "# assign the count per word from .lemma_ for .sents\n",
    "bow['lemma'] = dfs.sentence.apply(lambda x: sentence_process(x))\n",
    "for i in range(len(bow)):\n",
    "    ctr = Counter(bow.lemma[i])\n",
    "    for (j,f) in enumerate(fte):\n",
    "        bow.iat[i,j] = ctr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero dataframe length number of sentences and width number of words (1000)\n",
    "bw3 = pd.DataFrame(np.zeros((len(df3), len(fts))), columns=fts, dtype=int)\n",
    "# assign the count per word from .lemma_ for .sents\n",
    "bw3['lemma'] = df3.sentence.apply(lambda x: sentence_process(x))\n",
    "for i in range(len(bw3)):\n",
    "    ctr = Counter(bw3.lemma[i])\n",
    "    for (j,f) in enumerate(fts):\n",
    "        bw3.iat[i,j] = ctr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero dataframe length number of sentences and width number of words \n",
    "bw4 = pd.DataFrame(np.zeros((len(df4), len(ft4))), columns=ft4, dtype=int)\n",
    "# assign the count per word from .lemma_ for .sents\n",
    "bw4['lemma'] = df4.sentence.apply(lambda x: sentence_process(x))\n",
    "for i in range(len(bw4)):\n",
    "    ctr = Counter(bw4.lemma[i])\n",
    "    for (j,f) in enumerate(ft4):\n",
    "        bw4.iat[i,j] = ctr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it all adds up\n",
    "for i in range(0, len(bow), 50):\n",
    "    assert bow.loc[i][:-1].sum() == len([word for word in bow.lemma[i] if word in fte])\n",
    "# check that it all adds up\n",
    "for i in range(0, len(bw3), 50):\n",
    "    assert bw3.loc[i][:-1].sum() == len([word for word in bw3.lemma[i] if word in fts])\n",
    "for i in range(0, len(bw4), 50):\n",
    "    assert bw4.loc[i][:-1].sum() == len([word for word in bw4.lemma[i] if word in ft4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow[bow.columns[:-1]]\n",
    "y = dfs.who\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = bw3[bw3.columns[:-1]]\n",
    "X4 = bw4[bw4.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.where(df3.who == \"Carroll\", 1, 0)\n",
    "y2 = np.where(df3.who == \"Austen\",  1, 0)\n",
    "y3 = np.where(df4.who == \"Austen\",  1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "X1_trn, X1_tst, y1_trn, y1_tst = train_test_split(X3, y1, test_size=0.2, random_state=9)\n",
    "X2_trn, X2_tst, y2_trn, y2_tst = train_test_split(X3, y2, test_size=0.2, random_state=9)\n",
    "X3_trn, X3_tst, y3_trn, y3_tst = train_test_split(X4, y3, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier <a name=\"classify\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run classifiers with default hyperparameters for algorithim selection\n",
    "clf0 = LogisticRegression()\n",
    "clf1 = RandomForestClassifier(random_state=1)\n",
    "clf2 = SVC(random_state=1)\n",
    "\n",
    "N = ['logit', 'Random Forest', 'SVM classifier']\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 0  Result <a name=\"result1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logit': {'train': 0.9665653495440729, 'test': 0.9036144578313253},\n",
       " 'Random Forest': {'train': 0.9878419452887538, 'test': 0.8674698795180723},\n",
       " 'SVM classifier': {'train': 0.6960486322188449, 'test': 0.6746987951807228}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (i, n) in enumerate(N):\n",
    "    clf = eval('clf' + str(i))\n",
    "    clf.fit(X_trn, y_trn) \n",
    "    scores[n] = {'train': clf.score(X_trn, y_trn), 'test': clf.score(X_tst, y_tst)}\n",
    "     \n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 1  Result <a name=\"result2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.9657643312101911, 'test': 0.9426751592356688}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "scores = {}\n",
    "clf.fit(X1_trn, y1_trn)\n",
    "scores[1] = {'train': clf.score(X1_trn, y1_trn), 'test': clf.score(X1_tst, y1_tst)}\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.9585987261146497, 'test': 0.9299363057324841}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X2_trn, y2_trn)\n",
    "scores[2] = {'train': clf.score(X2_trn, y2_trn), 'test': clf.score(X2_tst, y2_tst)}\n",
    "scores[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X2_trn, y2_trn)\n",
    "scores[2] = {'train': clf.score(X2_trn, y2_trn), 'test': clf.score(X2_tst, y2_tst)}\n",
    "scores[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch   <a name=\"scratch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.loc[np.where(dfs.length <= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs.groupby('who').length.mean(), dfs.groupby('who').length.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = STOP_WORDS\n",
    "A1 = [tok for tok in doc1 if tok.is_alpha and not tok.is_stop]\n",
    "B1 = [a for a in A1 if a.lemma_ not in stops]\n",
    "C1 = [b.lemma_.lower() if b.pos_ != 'PROPN' else b.lower_ for b in B1]\n",
    "D1 = list(set(C1))\n",
    "print( len(A1), len(B1),len(C1), len(D1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [tok for for in A if a.is_alpha and not ]\n",
    "B = [tok for tok in docv if tok.lemma_ not in puncs and tok.lemma_ not in stops]\n",
    "C = [a.lemma_.lower() if a.pos_ != 'PROPN' else a.lower_ for a in A]\n",
    "D = set(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokens, ngram_range=(1,1))\n",
    "# bag of words\n",
    "tdf_vct = TfidfVectorizer(tokenizer = spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tdf_vct.fit_transform(all_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = tdf_vct.fit_transform(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging needs the model\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity dectection \n",
    "entities=[(i, i.label_, i.label) for i in nytimes.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Parsing\n",
    "for chunk in docp.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)\n",
    "displacy.render(docp, style=\"dep\", jupyter= True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vector Representation\n",
    "mango = toks[5]\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
