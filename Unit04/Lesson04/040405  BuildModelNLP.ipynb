{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Classifier\n",
    "1. [Objective](#obj)\n",
    "1. [Modules](#import)\n",
    "2. [Raw Text](#raw)\n",
    "3. [Tokenizer](#toke)\n",
    "4. [Feature](#feat)\n",
    "4. [Classifier](#classify)\n",
    "5. [Scratch](#scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective <a name=\"obj\"></a>\n",
    "\n",
    "Challenge: Build your own NLP model\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "- Data cleaning / processing / language parsing\n",
    "- Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "- Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "- Assess your models using cross-validation and determine whether one model performed better.\n",
    "- Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "- Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text <a name=\"raw\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all of the gutenberg texts\n",
    "texts = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt',\n",
    "         'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', \n",
    "         'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
    "         'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt',\n",
    "         'whitman-leaves.txt']\n",
    "authors = [re.split(r'\\W',text)[0] for text in texts]\n",
    "titles  = [re.split(r'\\W',text)[1] for text in texts]\n",
    "raws =    [gutenberg.raw(text)     for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning / processing / language parsing\n",
    "\n",
    "- Clean the raw text to minimize further processing after the tokenization.  Also reduces the text size allowing for more data while limited by a maximum char size of 100,000.\n",
    "\n",
    "- use SpaCy to parse the text to get list of lemmas for each sentence.  I could have just used sklearn text feature extraction, but it seems more \"black box\" to me.  Spacy allows greater opportunity to check the work product and re-process as required.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific cleaning re\n",
    "rx0 = re.compile(r'\\d+:\\d+')                           # remove chapter:verse annotation for the bible\n",
    "rx1 = re.compile(r'\\[.*\\]')                            # title at begining of all texts\n",
    "rx2 = re.compile(r'[A-Z]{3}')                          # 3 or more capitals, all\n",
    "rx3 = re.compile(r'Actus\\s+\\w+\\.')                     # shakespeare\n",
    "rx4 = re.compile(r'Scoena\\s+\\w+\\.')                    # shakespeare\n",
    "rx5 = re.compile(r'VOLUME\\s[IVX]+\\s+CHAPTER\\s+[IVX]+') # emma,\n",
    "rx6 = re.compile(r'Chapter \\d+', flags=re.I)           #persuasion, sense\n",
    "rx7 = re.compile(r'CHAPTER\\s+[IVX]+')                  # alice\n",
    "rx8 = re.compile(r'CHAPTER\\s+[IVX]+')                  # leaves\n",
    "rx9 = re.compile(r'\\s{2}')                             # two or more white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text specific cleaning\n",
    "raws = [rx1.sub(\"\", raw) for raw in raws]\n",
    "raws = [rx2.sub(\"\", raw) for raw in raws]\n",
    "raws[3] = rx0.sub(\"\", raws[3])\n",
    "raws[0] = rx5.sub(\"\", raws[0])\n",
    "raws[7] = rx7.sub(\"\", raws[7])\n",
    "raws[17] = rx8.sub(\"\", raws[17])\n",
    "for i in [1,2]:\n",
    "    raws[i] = rx6.sub(\"\", raws[i])\n",
    "for i in [14,15,16]:\n",
    "    raws[i] = rx3.sub(\"\", raws[i])\n",
    "raws = [rx9.sub(\"\", raw) for raw in raws]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guttenberg specific text cleaning\n",
    "def txt_clean(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "cleans = [txt_clean(raw) for raw in raws]\n",
    "# reduce size of each to under 100,000 to avoid allocation errors... \n",
    "# or actually just warnings about possible allocation errors\n",
    "for (i,clean) in enumerate(cleans):\n",
    "    cleans[i] = clean[len(clean) -100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer <a name=\"toke\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nlp model by sentence\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "dct = defaultdict(dict).fromkeys(titles)\n",
    "for (i,clean) in enumerate(cleans):\n",
    "    dct[titles[i]] = {'author':authors[i],'sents':None, 'title':titles[i]}\n",
    "    dct[titles[i]].update(sents=[S for S in nlp(clean).sents])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further process .sents eliminate ntlk stops, lemmatize, lower()\n",
    "# can then delete samples where result is []\n",
    "def sentence_process(S):\n",
    "    stops = STOP_WORDS\n",
    "    A = [s for s in S if s.is_alpha and not s.is_stop]\n",
    "    B = [a for a in A if a.lemma_ not in stops]\n",
    "    return [b.lemma_.lower() if b.pos_ != 'PROPN' else b.lemma_ for b in B]\n",
    "\n",
    "def text_df(sents, title, author):\n",
    "    S1 = pd.Series(sents)\n",
    "    S2 = np.repeat(title, len(S1)); S3 = np.repeat(author, len(S1))\n",
    "    df = pd.DataFrame([S1, S2, S3], index=['sentence','title', 'author']).T\n",
    "\n",
    "    df['lemma'] = df.sentence.apply(lambda x: sentence_process(x))\n",
    "    df['check'] = df.lemma.apply(lambda x: x.__len__())\n",
    "    df = df.drop(np.where(df.check == 0)[0], axis=0)\n",
    "    df = df.drop('check', axis=1)\n",
    "    df.index = range(len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [text_df(**dct[title]) for title in titles]\n",
    "df1 = pd.concat(dfs,axis=0)\n",
    "df1.index = (range(len(df1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>(Yet, let, me, not, be, too, hasty, ,, Long, i...</td>\n",
       "      <td>leaves</td>\n",
       "      <td>whitman</td>\n",
       "      <td>[let, hasty, long, live, sleep, blend, die, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14765</th>\n",
       "      <td>(If, we, go, anywhere, we, 'll, go, together, ...</td>\n",
       "      <td>leaves</td>\n",
       "      <td>whitman</td>\n",
       "      <td>[meet, happen, blither, learn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>(May, -, be, it, is, yourself, now, really, us...</td>\n",
       "      <td>leaves</td>\n",
       "      <td>whitman</td>\n",
       "      <td>[usher, true, song, know]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>(May, -, be, it, is, you, the, mortal, knob, r...</td>\n",
       "      <td>leaves</td>\n",
       "      <td>whitman</td>\n",
       "      <td>[mortal, knob, undo, turn, finally, Good, bye,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>(my, Fancy, .)</td>\n",
       "      <td>leaves</td>\n",
       "      <td>whitman</td>\n",
       "      <td>[Fancy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence   title   author  \\\n",
       "14764  (Yet, let, me, not, be, too, hasty, ,, Long, i...  leaves  whitman   \n",
       "14765  (If, we, go, anywhere, we, 'll, go, together, ...  leaves  whitman   \n",
       "14766  (May, -, be, it, is, yourself, now, really, us...  leaves  whitman   \n",
       "14767  (May, -, be, it, is, you, the, mortal, knob, r...  leaves  whitman   \n",
       "14768                                     (my, Fancy, .)  leaves  whitman   \n",
       "\n",
       "                                                   lemma  \n",
       "14764  [let, hasty, long, live, sleep, blend, die, di...  \n",
       "14765                     [meet, happen, blither, learn]  \n",
       "14766                          [usher, true, song, know]  \n",
       "14767  [mortal, knob, undo, turn, finally, Good, bye,...  \n",
       "14768                                            [Fancy]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features <a name=\"feat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "\n",
    "- Used bag of words and tf-idf (an engineer most have come up with that lame name) methods to create features.\n",
    "- sklearn CountVectorizer fit and transform for the bag of words sparse matrix and then TfidfTransformer to transform bag of words to the tf-idf sparse matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn does not take list of words, each sample back to text string of lemmas\n",
    "lemmas = [' '.join(x) for x in df1.lemma.tolist()]\n",
    "vectorizer = CountVectorizer(lowercase=False,min_df=0.002, max_df=0.50)\n",
    "X = vectorizer.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14769, 677)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfr = TfidfTransformer()\n",
    "Xt = tfr.fit_transform(X)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for 2 targets x 2 feature sets\n",
    "y1 = df1.title; y2 = df1.author\n",
    "X1_trn, X1_tst, y1_trn, y1_tst = train_test_split(X, y1,  test_size=0.2, random_state=9)\n",
    "X2_trn, X2_tst, y2_trn, y2_tst = train_test_split(X, y2,  test_size=0.2, random_state=9)\n",
    "X3_trn, X3_tst, y3_trn, y3_tst = train_test_split(Xt, y1, test_size=0.2, random_state=9)\n",
    "X4_trn, X4_tst, y4_trn, y4_tst = train_test_split(Xt, y2, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train / test data as tuples to more easily document initial 8 classifcations\n",
    "A = train_test_split(X,  y1,  test_size=0.2, random_state=9)\n",
    "B = train_test_split(X,  y2,  test_size=0.2, random_state=9)\n",
    "C = train_test_split(Xt, y1,  test_size=0.2, random_state=9)\n",
    "D = train_test_split(Xt, y2,  test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier <a name=\"classify\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check two different classifiers \n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "clf2 = RandomForestClassifier(n_estimators=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\chuck\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "dct = {'A':('title', 'BoW'), 'C':('title', 'tfidf'), 'B':('author', 'BoW'), 'D':('author', 'tfidf'), \n",
    "       1:'LogisticRegression', 2:'RandomForestClassifier'}\n",
    "for c in 'ABCD':\n",
    "    for i in range(1,3):\n",
    "        tup = eval(c); clf = eval('clf' + str(i))\n",
    "        clf.fit(tup[0], tup[2])\n",
    "        scores.append(pd.Series({'classifier': dct[i], 'target': dct[c][0], 'method': dct[c][1], \n",
    "                                 'train':clf.score(tup[0], tup[2]), 'test':clf.score(tup[1], tup[3])}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial look at accuracy scores\n",
    "Reviewing eight sets of results for the two targets (author, title), two NLP methods (bag of words and that poorly named tf-idf) and two classifiers.  As the Random Forest Classifier seems to be overfitting, continue with logit classifier and the author target.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>target</th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>title</td>\n",
       "      <td>BoW</td>\n",
       "      <td>0.629285</td>\n",
       "      <td>0.495261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>title</td>\n",
       "      <td>BoW</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.433649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>author</td>\n",
       "      <td>BoW</td>\n",
       "      <td>0.719255</td>\n",
       "      <td>0.633378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>author</td>\n",
       "      <td>BoW</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.561273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>title</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.599577</td>\n",
       "      <td>0.493230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>title</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.832924</td>\n",
       "      <td>0.433649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>author</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.687177</td>\n",
       "      <td>0.626608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>author</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.877782</td>\n",
       "      <td>0.573798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               classifier  target method     train      test\n",
       "0      LogisticRegression   title    BoW  0.629285  0.495261\n",
       "1  RandomForestClassifier   title    BoW  0.834025  0.433649\n",
       "2      LogisticRegression  author    BoW  0.719255  0.633378\n",
       "3  RandomForestClassifier  author    BoW  0.880152  0.561273\n",
       "4      LogisticRegression   title  tfidf  0.599577  0.493230\n",
       "5  RandomForestClassifier   title  tfidf  0.832924  0.433649\n",
       "6      LogisticRegression  author  tfidf  0.687177  0.626608\n",
       "7  RandomForestClassifier  author  tfidf  0.877782  0.573798"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_bow = LogisticRegressionCV(cv=5, random_state=0, solver='lbfgs', multi_class='auto', max_iter=200).fit(X,  y2)\n",
    "#clf_idf = LogisticRegressionCV(cv=5, random_state=0, solver='lbfgs', multi_class='auto', max_iter=200).fit(Xt, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Assess your models using cross-validation and determine whether one model performed better.\n",
    " - Continue with the bag of words based opun the cross validation score and try to improve the accuracy from 63% to 68%\n",
    " - Logit classifier does not converge at 300 iterations or with 'saga' solver and 'elastic net' penalty.  Score does not change with increased iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words cross validation score: 0.7239\n",
      "tf-idf cross validation score: 0.6970\n"
     ]
    }
   ],
   "source": [
    "print('Bag of words cross validation score: %.4f' % clf_bow.score(X, y2))\n",
    "print('tf-idf cross validation score: %.4f' % clf_idf.score(X, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "1. Check if PCA helps with convergence. Or I meant TruncatedSVD since it is a sparse matrix.\n",
    "  - At n_components=100 accuracy reduced from 63% to 55%. \n",
    "  - At n_components=300 the accuracy is 60%.  Plus the logit did converge with 127 iterations.\n",
    "  - Decomposition likely need for the next step  \n",
    "\n",
    "2. Increase the Bag of Words features.\n",
    " - increase bag of words features from 677 to 1400,  features then reduced by decomposition to 700.  Accuracy 66%\n",
    " - bag of words 1400 features then reduced by decomposition to 1000.  Accuracy 67%\n",
    " - bag of words 2485 features then reduced by decomposition to 1000.  Accuracy 68%\n",
    " \n",
    " A better approach would have been to add to the volume of text that was processed.  I stayed within the SpaCy default limit of 100,000 characters per doc for each of the tiles, but could have have processed the entire title with multiple iterations.  Or changed the setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7655132936626161"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier runs prior to increasing BoW feature number\n",
    "svd = TruncatedSVD(n_components=300, random_state=3)\n",
    "Xs = svd.fit_transform(X)\n",
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5995260663507109"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)\n",
    "clf.fit(X_trn, y_trn)\n",
    "clf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance ration: 0.8348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6844955991875423"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X1 features used to increase accuracy\n",
    "vectorizer = CountVectorizer(lowercase=False,min_df=0.0005, max_df=0.50)\n",
    "X1 = vectorizer.fit_transform(lemmas)\n",
    "svd = TruncatedSVD(n_components=1000, random_state=3)\n",
    "X1s = svd.fit_transform(X1)\n",
    "print('explained variance ration: %.4f' % svd.explained_variance_ratio_.sum())\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X1s, y2,  test_size=0.2, random_state=9)\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)\n",
    "clf.fit(X_trn, y_trn)\n",
    "clf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch   <a name=\"scratch\"></a>\n",
    "\n",
    "Save some of the superfluous code snippets that may be useful in the future. Plus any unit tests.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that samples still match up with features\n",
    "assert df1.shape[0] == X.shape[0]\n",
    "assert df1.shape[0] == Xt.shape[0]\n",
    "assert X.shape      == Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "clf1.fit(X3_trn, y3_trn)\n",
    "scores = {'train': clf1.score(X3_trn, y3_trn), 'test': clf1.score(X3_tst, y3_tst)}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It was all the service she could now render her poor friend; for as to any of that heroism of sentiment which might have prompted her to entreat him to transfer his affection from herself to Harriet, as infinitely the most worthy of the two or even the more simple sublimity of resolving to refuse him at once and for ever, without vouchsafing any motive, because he could not marry them both, Emma had it not."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sentence[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-PRON- be all the service -PRON- could now render -PRON- poor friend ; for as to any of that heroism of sentiment which may have prompt -PRON- to entreat -PRON- to transfer -PRON- affection from -PRON- to Harriet , as infinitely the most worthy of the two or even the more simple sublimity of resolve to refuse -PRON- at once and for ever , without vouchsafe any motive , because -PRON- could not marry -PRON- both , Emma have -PRON- not .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sentence[2].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad language feeling agitation doubt reluctance discouragement receive discouragement',\n",
       " 'time conviction glow attendant happiness time rejoice harriet secret escape resolve nee',\n",
       " 'service render poor friend heroism sentiment prompt entreat transfer affection harriet infinitely worthy simple sublimity resolve refuse vouchsafe motive marry emma',\n",
       " 'feel harriet pain contrition flight generosity run mad oppose probable reasonable enter brain']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sklearn.feature_extraction.text.CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’,\n",
    "strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’,\n",
    "ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, \n",
    "                                dtype=<class ‘numpy.int64’>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx4.sub('test',raws[14][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity dectection \n",
    "entities=[(i, i.label_, i.label) for i in nytimes.ents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
